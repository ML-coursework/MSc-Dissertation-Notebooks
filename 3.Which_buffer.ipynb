{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ee8953a-1a25-4c88-8c67-cbf3f93ff13d",
   "metadata": {},
   "source": [
    "# <u>Explanatory Logistic Regression</u> - cleaned and runnning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7bd2b-aabb-4f0b-b914-50586c5e3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python packages and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import statsmodels.formula.api as smf\n",
    "import warnings\n",
    "from folium.plugins import MarkerCluster\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from shapely.geometry import Point\n",
    "from branca.colormap import linear\n",
    "from patsy import dmatrix\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fdbf5-97f9-4a42-a562-1825c9af6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing log transformed and scaled buffer count (see 2.T_test notebook) \n",
    "df = gpd.read_file(\"df_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608f6c31-94d4-41fd-bfbe-52780e739d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3656e29b-702e-49c5-8551-c30a16d9ddb3",
   "metadata": {},
   "source": [
    "------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0688a1ec-d173-4912-9fa8-194f12725406",
   "metadata": {},
   "source": [
    "# Cleaning and examing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dad253-df28-4afe-94c4-9375857d4395",
   "metadata": {},
   "source": [
    "#### Creating a GeoDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9feb44-f309-42d5-939b-f71d1c4da205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure the Latitude and Longitude are numeric\n",
    "df[\"latitude\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "\n",
    "# Creating a Point geometry column\n",
    "df[\"Point geometry\"] = df.apply(lambda row: Point(row[\"longitude\"], row[\"latitude\"]), axis=1)\n",
    "\n",
    "# Making the GeoDataFrame\n",
    "gdf = gpd.GeoDataFrame(df, geometry=\"Point geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "# Preview\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80cb0b4-c1c3-4cdf-bf64-a7a1240a3f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new binary is_strandings column\n",
    "gdf['is_stranding'] = gdf['Data'].apply(lambda x: 1 if x == 'Strandings points' else 0)\n",
    "\n",
    "# Ensuring 'Buffer size' is treated as integer\n",
    "if gdf['Buffer size'].dtype == 'object':\n",
    "    gdf['Buffer size'] = gdf['Buffer size'].str.replace('m', '').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef220734-9672-4001-b895-43c20db7b6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking crs\n",
    "print(\"CRS:\", gdf.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051242bf-7363-48de-9cd4-4ae6a1a62523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null values across all buffers\n",
    "gdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a89d09a-5fab-4d99-8351-2c2fd42c3c9b",
   "metadata": {},
   "source": [
    "----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deda2e5-e949-4838-9e06-1a8ac476b650",
   "metadata": {},
   "source": [
    "## Explanatory Logistic Regression \n",
    "\n",
    "#### Preliminary t-tests indicated stronger performance with smaller buffer sizes, whereas predictive logistic regression models achieved higher accuracy with larger buffers. To refine the choice of buffer size, I next applied explanatory logistic regression to better evaluate the underlying relationships before selecting an appropriate scale for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1c174-99d3-4dc0-8958-e359c90f1f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting what data to use, creating a copy of the gdf to work with\n",
    "def run_logit_by_buffers(\n",
    "    gdf,\n",
    "    buffers=(500, 1000, 1500, 3000, 5000),\n",
    "    predictors=('Building_trans', 'Road_trans', 'Bathymetry_trans', 'Other_points_trans'),\n",
    "    buffer_col='Buffer size',\n",
    "    data_col='Data',\n",
    "    target_label='Strandings points'):\n",
    "    df = gdf.copy()\n",
    "\n",
    "     # Making sure predictors are numeric\n",
    "    for col in predictors:\n",
    "         df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    # Dropping any null values for predictors/target\n",
    "    df = df.dropna(subset=list(predictors) + ['is_stranding'])\n",
    "\n",
    "    # Creating containers\n",
    "    model_rows = []\n",
    "    or_rows = []\n",
    "    vif_rows = []\n",
    "    results = {}\n",
    "    # looping over the buffers and fitting the logit model per buffer\n",
    "    for b in buffers:\n",
    "        subset = df[df[buffer_col] == b].copy()\n",
    "        if subset.empty:\n",
    "            model_rows.append({'Buffer': b, 'n': 0, 'AIC': np.nan, 'Pseudo_R2': np.nan})\n",
    "            continue\n",
    "        # bulding X from the predictors and intercept, setting y as 'is_stranding'\n",
    "        X = subset[list(predictors)]\n",
    "        X = sm.add_constant(X, has_constant='add')\n",
    "        y = subset['is_stranding']\n",
    "\n",
    "        # Skipping an drecord any NaNs to prevent degenerate MLE\n",
    "        if X.shape[0] <= X.shape[1]:\n",
    "            model_rows.append({'Buffer': b, 'n': len(subset), 'AIC': np.nan, 'Pseudo_R2': np.nan})\n",
    "            continue\n",
    "        # Storing results \n",
    "        try:\n",
    "            model = sm.Logit(y, X)\n",
    "            res = model.fit(disp=False, maxiter=100)\n",
    "        except Exception as e:\n",
    "            # model could fail with complete separation, etc.\n",
    "            model_rows.append({'Buffer': b, 'n': len(subset), 'AIC': np.nan, 'Pseudo_R2': np.nan, 'error': str(e)})\n",
    "            continue\n",
    "        \n",
    "        results[b] = res\n",
    "\n",
    "        # Collecting per-buffer model stats\n",
    "        model_rows.append({\n",
    "            'Buffer': b,\n",
    "            'n': int(len(subset)),\n",
    "            'AIC': float(res.aic),\n",
    "            'Pseudo_R2': float(res.prsquared),\n",
    "            'LL': float(res.llf)})\n",
    "\n",
    "        # Computing odds ratios (exp(coef)) with 95% CI and p-values\n",
    "        conf = res.conf_int()\n",
    "        conf.columns = ['CI_low', 'CI_high']\n",
    "        coef = res.params.rename('coef')\n",
    "        pvals = res.pvalues.rename('pval')\n",
    "        or_df = pd.concat([coef, conf, pvals], axis=1).reset_index().rename(columns={'index':'Variable'})\n",
    "        or_df['Buffer'] = b\n",
    "        or_df['OR'] = np.exp(or_df['coef'])\n",
    "        or_df['CI_low'] = np.exp(or_df['CI_low'])\n",
    "        or_df['CI_high'] = np.exp(or_df['CI_high'])\n",
    "        or_rows.append(or_df[['Buffer','Variable','OR','CI_low','CI_high','pval']])\n",
    "\n",
    "        # computing VIF for multicollinearity\n",
    "        vif = pd.DataFrame({\n",
    "            'Buffer': b,\n",
    "            'Feature': X.columns,\n",
    "            'VIF': [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]})\n",
    "        vif_rows.append(vif)\n",
    "    #Returning tidy tables and raw models\n",
    "    summary_df = pd.DataFrame(model_rows).sort_values('Buffer').reset_index(drop=True)\n",
    "    or_table = pd.concat(or_rows, ignore_index=True) if or_rows else pd.DataFrame()\n",
    "    vif_table = pd.concat(vif_rows, ignore_index=True) if vif_rows else pd.DataFrame()\n",
    "\n",
    "    return summary_df, or_table, vif_table, results\n",
    "\n",
    "summary, ORs, VIFs, models = run_logit_by_buffers(gdf)\n",
    "# Prings per-buffer n, AIC, pseudo-R2\n",
    "print(summary)  \n",
    "# odds ratios with 95% CI & p-values\n",
    "print(ORs.head(25))\n",
    "# multicollinearity diagnostics\n",
    "print(VIFs.head(25)) \n",
    "\n",
    "# Example: see full statsmodels summary for which a single buffer size, to take a more indepth look\n",
    "print(models[500].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c64df-816f-4b0e-b192-efd9a19ee0b3",
   "metadata": {},
   "source": [
    "#### Key points: smaller buffers (500m - 1000m) produce better fitting, more explanatory models with highest pseudo-R² (500m ~0.25) and lower AIC (500m ~52,417). Roads and buildings are the strongest positive predictors of strandings, other points are influential and multicollinearity isn't a concern. \n",
    "\n",
    "#### Deeper dive on 500m buffer shows: Strandings are most strongly driven by spatial clustering and human presence, while bathymetry reduces risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea2002-93ec-4298-9c28-f1c3d4049e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all outputs to one text file for your appendix\n",
    "with open(\"Method_results_images/Logistic_Regression_summary.txt\", \"w\") as f:\n",
    "    # Write summary table\n",
    "    f.write(\"=== Summary ===\\n\")\n",
    "    f.write(summary.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "with open(\"Method_results_images/Logistic_Regression_Odds_Ratio.txt\", \"w\") as f:    \n",
    "    f.write(\"=== Odds Ratios ===\\n\")\n",
    "    f.write(ORs.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "with open(\"Method_results_images/Logistic_Regression_VIF_Table.txt\", \"w\") as f:    \n",
    "    f.write(\"=== VIF Table ===\\n\")\n",
    "    f.write(VIFs.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "\n",
    "with open(\"Method_results_images/Logistic_Regression_Full_Model_Summary.txt\", \"w\") as f:    \n",
    "    f.write(\"=== Full Model Summary (1000 m) ===\\n\")\n",
    "    f.write(models[1000].summary().as_text())\n",
    "\n",
    "print(\"Saved everything to method results images.txt\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d5cd6b-fb74-4f79-a4ef-29a5f9c251c6",
   "metadata": {},
   "source": [
    "## Visulising the results\n",
    "\n",
    "#### Model fit summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d66bf8-bfed-4813-aadd-64ff1df241b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing warnings for aesthetics\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "# Plotting, displaying and visulising AIC and Pseudo-R graphs\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "sns.lineplot(data=summary, x=\"Buffer\", y=\"AIC\", marker=\"o\", ax=ax[0])\n",
    "ax[0].set_title(\"AIC vs Buffer Size\")\n",
    "sns.lineplot(data=summary, x=\"Buffer\", y=\"Pseudo_R2\", marker=\"o\", ax=ax[1])\n",
    "ax[1].set_title(\"Pseudo-R² vs Buffer Size\")\n",
    "plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Method_results_images/Logistic_Regression_3_AIC_Pseudo_R_vs_buffer_size.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c66b9b4-7ea5-4853-82c3-ee746e870076",
   "metadata": {},
   "source": [
    "#### AIC vs buffer size: increases as buffer size increases, lower AIC = better fit so 500m buffer performs best, while larger buffers perform worse. \n",
    "#### Pseudo-R²: decreases as buffersize increases, 500m buffer performs best again, explaining the most variation in strandings presence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaf3b9a-0138-4e81-8f87-d6eac1d0f8cb",
   "metadata": {},
   "source": [
    "#### Odds ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bc2a7f-3c8b-4989-941c-a9f1d52582ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying up the \n",
    "name_map = {\n",
    "    'Building_trans':'Buildings',\n",
    "    'Road_trans':'Roads',\n",
    "    'Bathymetry_trans':'Bathymetry',\n",
    "    'Other_points_trans':'Other strandings'}\n",
    "\n",
    "ORs['Predictor'] = ORs['Variable'].map(name_map).fillna(ORs['Variable'])\n",
    "\n",
    "# Make sure Buffer is numeric & ordered\n",
    "ORs['Buffer'] = pd.to_numeric(ORs['Buffer'], errors='coerce')\n",
    "buffer_order = [500, 1000, 1500, 3000, 5000]\n",
    "ORs['Buffer'] = pd.Categorical(ORs['Buffer'], categories=buffer_order, ordered=True)\n",
    "\n",
    "# Custom palette\n",
    "palette = {\n",
    "    'Buildings': '#1f77b4',\n",
    "    'Roads': '#ff7f0e',\n",
    "    'Bathymetry': '#2ca02c',\n",
    "    'Other strandings': '#d62728'}\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(9,6))\n",
    "sns.lineplot(data=ORs[ORs['Variable']!='const'],\n",
    "             x='Buffer', y='OR', hue='Predictor',\n",
    "             palette=palette, marker='o')\n",
    "plt.yscale('log')\n",
    "plt.axhline(1, color='gray', ls='--', lw=1.5)\n",
    "plt.xlabel('Buffer size')\n",
    "plt.ylabel('Odds ratio (log scale)')\n",
    "plt.title('Odds ratios across buffers')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"Method_results_images/Logistic_Regression_Odds_Ratios_buffer_size.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e151ec-0d82-4226-8e94-7834dc418405",
   "metadata": {},
   "source": [
    "#### Odds ratio: local factors, bathymetry and other strandings dominate at smaller scales, human presence (roads, buildings) remain consistently important wiht roads increasing in finfluence at larger scales. The strongest predictor is presence of other strandings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe6764-3f0f-4053-a7bc-db9cc5a11784",
   "metadata": {},
   "source": [
    "#### VIFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da50c59-9159-4c0f-baa9-ccc26645d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up the legend names\n",
    "name_map = {\n",
    "    \"Building_trans\": \"Buildings\",\n",
    "    \"Road_trans\": \"Roads\",\n",
    "    \"Bathymetry_trans\": \"Bathymetry\",\n",
    "    \"Other_points_trans\": \"Other strandings\"}\n",
    "\n",
    "# Appling mapping to new column\n",
    "VIFs['Predictor'] = VIFs['Feature'].map(name_map).fillna(VIFs['Feature'])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.lineplot(data=VIFs[VIFs[\"Predictor\"]!=\"const\"], \n",
    "             x=\"Buffer\", y=\"VIF\", hue=\"Predictor\", marker=\"o\")\n",
    "\n",
    "plt.axhline(5, ls=\"--\", c=\"red\", label=\"VIF=5 (multicollinearity threshold)\")\n",
    "plt.title(\"VIF by Buffer Size\")\n",
    "plt.legend(title=\"Predictor\")\n",
    "plt.savefig(\"Method_results_images/Logistic_Regression_VIF_buffer_size.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d561c496-4749-4f23-95ff-91d0984556a4",
   "metadata": {},
   "source": [
    "#### VIF: Multicollinearity is not a concern in the models, roads and buildings show (expected) correlation, especially as the buffers get bigger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c0b041-a8d0-4f2d-97bf-2d020f74ee0d",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a382f-ad4b-46b1-abf1-c608ed4c331a",
   "metadata": {},
   "source": [
    "# Residual analysis\n",
    "\n",
    "#### Small (500m buffers) vs large (3000m buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df566061-0898-411c-9e5f-5773877e7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_diagnostics(res):\n",
    "    \"\"\"Convenience: return fitted, pearson residuals, leverage, Cook's D.\"\"\"\n",
    "    infl = res.get_influence()\n",
    "    return (res.fittedvalues,\n",
    "            res.resid_pearson,\n",
    "            infl.hat_matrix_diag,\n",
    "            infl.cooks_distance[0])\n",
    "\n",
    "def plot_residuals_clean(res, buffer_size, label_mode=\"none\", top_k=10, quantile=0.99):\n",
    "    \"\"\"\n",
    "    Residuals vs Fitted + Leverage vs Residuals for a statsmodels Logit result.\n",
    "    label_mode: \"none\" (default), \"topk\", or \"quantile\"\n",
    "    \"\"\"\n",
    "    fitted, pearson, leverage, cooks_d = residual_diagnostics(res)\n",
    "\n",
    "    # Residuals vs Fitted \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(fitted, pearson, s=8, alpha=0.5)\n",
    "    plt.axhline(0, color='red', ls='--', lw=1)\n",
    "    plt.xlabel(\"Fitted values\"); plt.ylabel(\"Pearson residuals\")\n",
    "    plt.title(f\"Residuals vs Fitted ({buffer_size} m)\")\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # Leverage vs Residuals\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(leverage, pearson, s=8, alpha=0.5)\n",
    "    plt.axhline(0, color='red', ls='--', lw=1)\n",
    "    plt.xlabel(\"Leverage\"); plt.ylabel(\"Pearson residuals\")\n",
    "    plt.title(f\"Leverage vs Residuals ({buffer_size} m)\")\n",
    "\n",
    "    if label_mode == \"topk\":\n",
    "        idx = np.argsort(cooks_d)[-top_k:]\n",
    "    elif label_mode == \"quantile\":\n",
    "        thr = np.quantile(cooks_d, quantile)\n",
    "        idx = np.where(cooks_d >= thr)[0]\n",
    "    else:\n",
    "        idx = []\n",
    "\n",
    "    # Only annotate a *few* influential points \n",
    "    for i in idx:\n",
    "        plt.annotate(str(int(i)), (leverage[i], pearson[i]),\n",
    "                     xytext=(2,2), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "def plot_residuals_highlight(res, buffer_size, quantile=0.99):\n",
    "    \"\"\"Highlight (don’t label) the most influential observations by Cook’s D.\"\"\"\n",
    "    _, pearson, leverage, cooks_d = residual_diagnostics(res)\n",
    "    thr = np.quantile(cooks_d, quantile)\n",
    "    hi = cooks_d >= thr\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(leverage[~hi], pearson[~hi], s=8, alpha=0.35, label=\"Others\")\n",
    "    plt.scatter(leverage[hi], pearson[hi], s=24, alpha=0.9, edgecolor='k',\n",
    "                label=f\"Top {int((1-quantile)*100)}% Cook's D\")\n",
    "    plt.axhline(0, color='red', ls='--', lw=1)\n",
    "    plt.xlabel(\"Leverage\"); plt.ylabel(\"Pearson residuals\")\n",
    "    plt.title(f\"Leverage vs Residuals ({buffer_size} m)\")\n",
    "    plt.legend(frameon=False)\n",
    "    plt.tight_layout(); \n",
    "    plt.savefig(\"Method_results_images/Logistic_Regression_500m_Pearson_residuals.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3a2378-b516-440a-bf4e-b59994d5db0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals_clean(models[500], 500, label_mode=\"none\")\n",
    "plot_residuals_clean(models[3000], 3000, label_mode=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213901c2-0471-4a2f-9500-962feb00cb69",
   "metadata": {},
   "source": [
    "#### Plots indicate that logistic regression model performes more consistently at 500m buffers size compared to the 3000m. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a52dab0-5ad3-4bed-8c54-9ae5b8a7563b",
   "metadata": {},
   "source": [
    "#### Filters only the high deviance residuals (> 2)) for 500m buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12d69a6-a5a2-41cf-9c4e-f0d509a6ef54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "BUF_M = 500                     \n",
    "PREDICTORS = ['Building_trans','Road_trans','Bathymetry_trans','Other_points_trans']\n",
    "THRESH = 2.0                  \n",
    "USE_DEV = True                 \n",
    "SAVE_HTML = True             \n",
    "\n",
    "# Creating a subset 500\n",
    "df = gdf.copy()\n",
    "\n",
    "# normalizing buffer to numeric meters \n",
    "if df['Buffer size'].dtype == 'object':\n",
    "    df['_buf_m'] = df['Buffer size'].astype(str).str.replace('m', '', regex=False).astype(int)\n",
    "else:\n",
    "    df['_buf_m'] = df['Buffer size']\n",
    "\n",
    "sub = df.loc[df['_buf_m'] == BUF_M].copy()\n",
    "\n",
    "# making sure geometry is WGS84 for mapping\n",
    "if 'geometry' not in sub.columns and 'Point geometry' in sub.columns:\n",
    "    sub = sub.rename(columns={'Point geometry':'geometry'})\n",
    "sub = gpd.GeoDataFrame(sub, geometry='geometry', crs=gdf.crs)\n",
    "sub = sub.to_crs(4326)\n",
    "\n",
    "# labelling target and keeping usable rows\n",
    "sub['is_stranding'] = (sub['Data'] == 'Strandings points').astype(int)\n",
    "for c in PREDICTORS:\n",
    "    sub[c] = pd.to_numeric(sub[c], errors='coerce')\n",
    "sub = sub.dropna(subset=PREDICTORS + ['is_stranding'])\n",
    "\n",
    "# Fitting logit on 500 m \n",
    "X = sm.add_constant(sub[PREDICTORS], has_constant='add')\n",
    "y = sub['is_stranding'].values\n",
    "res = sm.Logit(y, X).fit(disp=False, maxiter=100)\n",
    "\n",
    "# Residuals\n",
    "def binomial_deviance_residual(y, mu, eps=1e-12):\n",
    "    mu = np.clip(mu, eps, 1 - eps)\n",
    "    # y is 0/1\n",
    "    term1 = y * np.log(np.clip((y + eps) / mu, eps, None))\n",
    "    term2 = (1 - y) * np.log(np.clip((1 - y + eps) / (1 - mu), eps, None))\n",
    "    return np.sign(y - mu) * np.sqrt(2.0 * (term1 + term2))\n",
    "\n",
    "mu = res.predict(X)\n",
    "if USE_DEV:\n",
    "    resid = binomial_deviance_residual(y, mu)\n",
    "else:\n",
    "    resid = res.resid_pearson\n",
    "\n",
    "sub['residual'] = resid\n",
    "sub['_abs_resid'] = np.abs(sub['residual'])\n",
    "\n",
    "# Filtering high residuals\n",
    "hi = sub.loc[sub['_abs_resid'] > THRESH].copy()\n",
    "\n",
    "# nothing to plot guard\n",
    "if hi.empty:\n",
    "    print(\"No points exceed the residual threshold; try lowering THRESH.\")\n",
    "else:\n",
    "    # Map\n",
    "    center = [hi.geometry.y.mean(), hi.geometry.x.mean()] if len(hi) else [55, -3]\n",
    "    m = folium.Map(location=center, zoom_start=5, tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "    vmin, vmax = float(hi['residual'].min()), float(hi['residual'].max())\n",
    "    cmap = linear.RdBu_11.scale(vmin, vmax)\n",
    "    cmap.caption = f\"{'Deviance' if USE_DEV else 'Pearson'} residual (|res|>{THRESH})\"\n",
    "\n",
    "    def fmt(v, d=2):\n",
    "        try:\n",
    "            x = float(v); \n",
    "            return \"NA\" if pd.isna(x) else f\"{x:.{d}f}\"\n",
    "        except Exception:\n",
    "            return \"NA\"\n",
    "\n",
    "    for _, r in hi.iterrows():\n",
    "        popup_html = (\n",
    "            f\"<b>{'Deviance' if USE_DEV else 'Pearson'} residual:</b> {fmt(r['residual'])}<br>\"\n",
    "            + (f\"<b>Building count:</b> {fmt(r.get('Building count', np.nan), 0)}<br>\" if 'Building count' in r else \"\")\n",
    "            + (f\"<b>Road length:</b> {fmt(r.get('Road length', np.nan))} m\" if 'Road length' in r else \"\"))\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            [r.geometry.y, r.geometry.x],\n",
    "            radius=5,\n",
    "            color=cmap(r['residual']),\n",
    "            fill=True, fill_color=cmap(r['residual']), fill_opacity=0.85,\n",
    "            popup=folium.Popup(popup_html, max_width=300),\n",
    "        ).add_to(m)\n",
    "\n",
    "title_html = \"\"\"\n",
    "<div style=\"\n",
    "    position: absolute;\n",
    "    top: 12px;\n",
    "    left: 64px;              /* nudged right from 12px -> 64px */\n",
    "    z-index: 9999;\n",
    "    background: white;\n",
    "    padding: 6px 10px;\n",
    "    border: 1px solid #777;\n",
    "    border-radius: 6px;\n",
    "    font-weight: 600;\n",
    "    box-shadow: 0 1px 3px rgba(0,0,0,0.2);\n",
    "    pointer-events: none;    /* still lets you click the zoom buttons */\n",
    "\">\n",
    "High deviance residuals (&gt; 2) for 500m buffer\n",
    "</div>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "cmap.add_to(m)\n",
    "\n",
    "display(m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76e1c49-c70c-45a2-aa61-6414094294ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"Method_results_images/Logistic_Regression_High_Deviance_Residuals_Map_500m.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a6bd3b-ee43-4aec-83ed-9b25d35ba111",
   "metadata": {},
   "source": [
    "#### Saved as a link with Netlify [High deviance residuals (> 2)) for 500m buffer](https://high-deviance-residuals-500m.netlify.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2ec776-1432-4c7e-b471-f61bf2feda26",
   "metadata": {},
   "source": [
    "#### Filters only the high deviance residuals (> 2)) for 3000m buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8d8c4f-12d0-4e61-b37e-5cc054a0e647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "BUF_M = 3000                    \n",
    "PREDICTORS = ['Building_trans','Road_trans','Bathymetry_trans','Other_points_trans']\n",
    "THRESH = 2.0                  \n",
    "USE_DEV = True                 \n",
    "SAVE_HTML = True             \n",
    "\n",
    "# Creating a subset 500\n",
    "df = gdf.copy()\n",
    "\n",
    "# normalizing buffer to numeric meters \n",
    "if df['Buffer size'].dtype == 'object':\n",
    "    df['_buf_m'] = df['Buffer size'].astype(str).str.replace('m', '', regex=False).astype(int)\n",
    "else:\n",
    "    df['_buf_m'] = df['Buffer size']\n",
    "\n",
    "sub = df.loc[df['_buf_m'] == BUF_M].copy()\n",
    "\n",
    "# making sure geometry is WGS84 for mapping\n",
    "if 'geometry' not in sub.columns and 'Point geometry' in sub.columns:\n",
    "    sub = sub.rename(columns={'Point geometry':'geometry'})\n",
    "sub = gpd.GeoDataFrame(sub, geometry='geometry', crs=gdf.crs)\n",
    "sub = sub.to_crs(4326)\n",
    "\n",
    "# labelling target and keeping usable rows\n",
    "sub['is_stranding'] = (sub['Data'] == 'Strandings points').astype(int)\n",
    "for c in PREDICTORS:\n",
    "    sub[c] = pd.to_numeric(sub[c], errors='coerce')\n",
    "sub = sub.dropna(subset=PREDICTORS + ['is_stranding'])\n",
    "\n",
    "# Fitting logit on 500 m \n",
    "X = sm.add_constant(sub[PREDICTORS], has_constant='add')\n",
    "y = sub['is_stranding'].values\n",
    "res = sm.Logit(y, X).fit(disp=False, maxiter=100)\n",
    "\n",
    "# Residuals\n",
    "def binomial_deviance_residual(y, mu, eps=1e-12):\n",
    "    mu = np.clip(mu, eps, 1 - eps)\n",
    "    # y is 0/1\n",
    "    term1 = y * np.log(np.clip((y + eps) / mu, eps, None))\n",
    "    term2 = (1 - y) * np.log(np.clip((1 - y + eps) / (1 - mu), eps, None))\n",
    "    return np.sign(y - mu) * np.sqrt(2.0 * (term1 + term2))\n",
    "\n",
    "mu = res.predict(X)\n",
    "if USE_DEV:\n",
    "    resid = binomial_deviance_residual(y, mu)\n",
    "else:\n",
    "    resid = res.resid_pearson\n",
    "\n",
    "sub['residual'] = resid\n",
    "sub['_abs_resid'] = np.abs(sub['residual'])\n",
    "\n",
    "# Filtering high residuals\n",
    "hi = sub.loc[sub['_abs_resid'] > THRESH].copy()\n",
    "\n",
    "# nothing to plot guard\n",
    "if hi.empty:\n",
    "    print(\"No points exceed the residual threshold; try lowering THRESH.\")\n",
    "else:\n",
    "    # Map\n",
    "    center = [hi.geometry.y.mean(), hi.geometry.x.mean()] if len(hi) else [55, -3]\n",
    "    m = folium.Map(location=center, zoom_start=5, tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "    vmin, vmax = float(hi['residual'].min()), float(hi['residual'].max())\n",
    "    cmap = linear.RdBu_11.scale(vmin, vmax)\n",
    "    cmap.caption = f\"{'Deviance' if USE_DEV else 'Pearson'} residual (|res|>{THRESH})\"\n",
    "\n",
    "    def fmt(v, d=2):\n",
    "        try:\n",
    "            x = float(v); \n",
    "            return \"NA\" if pd.isna(x) else f\"{x:.{d}f}\"\n",
    "        except Exception:\n",
    "            return \"NA\"\n",
    "\n",
    "    for _, r in hi.iterrows():\n",
    "        popup_html = (\n",
    "            f\"<b>{'Deviance' if USE_DEV else 'Pearson'} residual:</b> {fmt(r['residual'])}<br>\"\n",
    "            + (f\"<b>Building count:</b> {fmt(r.get('Building count', np.nan), 0)}<br>\" if 'Building count' in r else \"\")\n",
    "            + (f\"<b>Road length:</b> {fmt(r.get('Road length', np.nan))} m\" if 'Road length' in r else \"\"))\n",
    "        \n",
    "        folium.CircleMarker(\n",
    "            [r.geometry.y, r.geometry.x],\n",
    "            radius=5,\n",
    "            color=cmap(r['residual']),\n",
    "            fill=True, fill_color=cmap(r['residual']), fill_opacity=0.85,\n",
    "            popup=folium.Popup(popup_html, max_width=300),\n",
    "        ).add_to(m)\n",
    "\n",
    "title_html = \"\"\"\n",
    "<div style=\"\n",
    "    position: absolute;\n",
    "    top: 12px;\n",
    "    left: 64px;              /* nudged right from 12px -> 64px */\n",
    "    z-index: 9999;\n",
    "    background: white;\n",
    "    padding: 6px 10px;\n",
    "    border: 1px solid #777;\n",
    "    border-radius: 6px;\n",
    "    font-weight: 600;\n",
    "    box-shadow: 0 1px 3px rgba(0,0,0,0.2);\n",
    "    pointer-events: none;    /* still lets you click the zoom buttons */\n",
    "\">\n",
    "High deviance residuals (&gt; 2) for 500m buffer\n",
    "</div>\n",
    "\"\"\"\n",
    "m.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "cmap.add_to(m)\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f6869-b466-4e4d-a048-9c5908e4d61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save(\"Method_results_images/Logistic_Regression_High_Deviance_Residuals_Map_3000m.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff111d5-7c5a-4745-a51e-7a58e92281d8",
   "metadata": {},
   "source": [
    "#### Saved as a link with Netlify [High deviance residuals (> 2)) for 3000m buffer](https://high-deviance-residuals-3000m.netlify.app/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0a1db-2e5c-4d0a-b929-9ac1995643df",
   "metadata": {},
   "source": [
    "# Continues in 5. Under-reported Strandings Zones notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
